# Databricks Lakeflow â€“ Hands-On Implementation

This repository contains the source code, sample data, and setup instructions for the hands-on implementation described in the Medium article:  
**Databricks Lakeflow: A Unified Data Engineering Solution**

## ğŸ“Œ Overview

Databricks Lakeflow is a unified data engineering solution that simplifies the ingestion, transformation, and orchestration of both batch and streaming data pipelines. This repo demonstrates how to build a complete Lakeflow pipeline using:

- Full-load ingestion with materialized views
- Incremental ingestion using streaming tables and Auto Loader
- Change Data Capture (CDC) and Slowly Changing Dimensions (SCD)
- Stream-static joins and aggregations
- Medallion Architecture (Bronze, Silver, Gold layers)

---

## ğŸ“ Repository Structure (lakeflow/)

```
â”œâ”€â”€ sample_data/
â”‚   â”œâ”€â”€ config_data_v1.csv
â”‚   â”œâ”€â”€ config_data_v2.csv
â”‚   â”œâ”€â”€ transactions_v1.csv
â”‚   â””â”€â”€ transactions_v2.csv
â”‚
â”œâ”€â”€ ldp_pipeline/transformations/
â”‚   â”œâ”€â”€ agg_country_stats.sql
â”‚   â”œâ”€â”€ config_city_mv.sql
â”‚   â”œâ”€â”€ transaction_bronze_flow.sql
â”‚   â”œâ”€â”€ transaction_clean_flow.sql
â”‚   â”œâ”€â”€ transaction_scd1_flow.sql
â”‚   â”œâ”€â”€ transaction_scd2_flow.sql
â”‚   â”œâ”€â”€ mapped_transaction_flow.sql
â”‚   â””â”€â”€ country_stats.sql
â”‚
â”œâ”€â”€ ldp_pipeline/explorations/
â”‚   â””â”€â”€ setup_notebook.py
â”‚
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. Upload Sample Data
   Use the ldp_pipeline/explorations/setup_notebook.py to perform setup activities such as uploading CSV files to a Databricks volume.

### 2. Run the Pipeline
   Use the SQL files in the ldp_pipeline/transformations/ directory to define and run your Lakeflow declarative pipeline.

## ğŸ™Œ Acknowledgments
This project is inspired by Databricks' Lakeflow GA announcement at the Data + AI Summit 2025.
